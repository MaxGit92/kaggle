{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle classif driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utile pour voir si la ram tient bon le chargement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import psutil\n",
    "#psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fonctions pour charger les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im(path):\n",
    "    # Load as grayscale\n",
    "    img = cv2.imread(path, 0)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (128, 96))\n",
    "    return resized\n",
    "\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('imgs','train', 'c'+str(j), '*.jpg')\n",
    "        print path\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            img = get_im(fl)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "    return X_train, y_train\n",
    "\n",
    "def load_test():\n",
    "    print('Read test images')\n",
    "    path = os.path.join('imgs', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total+=1\n",
    "        if(total%thr==0):\n",
    "            print str(total) + \" readen images on \" + str(len(files)) + \".\"\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement/enregistrement des données <br\\>\n",
    "A décommenter et exécuter uniquement pour charger et enregistrer les images. (Permet ensuite de charge BEAUCOUP plus rapidement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train, y_train = load_train()\\n\\nimport shelve\\nd = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/train')\\nd['X_train'] = X_train\\nd['y_train'] = y_train\\nd.close()\\n\\ndel X_train\\ndel y_train\\n\\nX_test, X_test_id = load_test()\\n\\nd = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/test')\\nd['X_test'] = X_test\\nd['X_test_id'] = X_test_id\\nd.close()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "##Enregistrement des données##\n",
    "##############################\n",
    "'''\n",
    "X_train, y_train = load_train()\n",
    "\n",
    "import shelve\n",
    "d = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/train')\n",
    "d['X_train'] = X_train\n",
    "d['y_train'] = y_train\n",
    "d.close()\n",
    "\n",
    "del X_train\n",
    "del y_train\n",
    "\n",
    "X_test, X_test_id = load_test()\n",
    "\n",
    "d = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/test')\n",
    "d['X_test'] = X_test\n",
    "d['X_test_id'] = X_test_id\n",
    "d.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques fonctions utiles\n",
    "notamment pour créer la submisson ou d'avoir un train set et test(validation) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def split_validation_set_with_hold_out(train, target, test_size):\n",
    "    random_state = 51\n",
    "    train, X_test, target, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, X_holdout, y_train, y_test, y_holdout\n",
    "\n",
    "def create_submission(predictions, test_id, loss):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = str(round(loss, 6)) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)\n",
    "\n",
    "def mlogloss(target, pred):\n",
    "    score=0.0\n",
    "    for i in range(len(pred)):\n",
    "        pp = pred[i]\n",
    "        for j in range(len(pp)):\n",
    "            prob = pp[j]\n",
    "            if(prob < 1e-15):\n",
    "                prob = 1e-15\n",
    "            score += target[i][j] * math.log(prob)\n",
    "    return -score/len(pred)\n",
    "\n",
    "def validate_holdout(model, holdout, target):\n",
    "    predictions = model.predict(holdout, batch_size=128, verbose=1)\n",
    "    score = log_loss(target, predictions)\n",
    "    print('Score log_loss: ', score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nest_batch permet d'entrainer par batch nos images, nous pouvons faire varier le nombre d'images que nous donnons en entrée.<br\\>\n",
    "<br\\>\n",
    "oneHotEncoding transforme nos sortie y en onehot (si nous avons des sorties telles que 0,1,2,0 nous aurons maintenant [1,0,0], [0,1,0], [0,0,1], [1,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size):\n",
    "    inds = np.random.choice(len(X), batch_size, replace=False)\n",
    "    return X[inds], y[inds]\n",
    "\n",
    "def oneHotEncoding(y, nb_classes):\n",
    "    res = np.zeros((y.shape[0], nb_classes))\n",
    "    res[np.arange(y.shape[0]), y] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow\n",
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nos images sont de tailles 96*128*1\n",
    "X = tf.placeholder(tf.float32, shape=[None, 96, 128])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Permet de créer les poids et les biais de notre réseau de neurones\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=1e-2)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facilite la création des convolution et des pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = input shape 4-D [batch, in_height, in_width, in_channels] (in_channels=number of colors)\n",
    "# w = filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "def conv2d(X, w):\n",
    "    return tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 is the number of input channel, here 1 because it is white and black\n",
    "# 32 is the number of output channel\n",
    "# 3 and 3 determine the two dimensions of the patch\n",
    "w_conv1 = weight_variable([3, 3, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons reshape X pour que les entrées soient en dimension 4 et accépter par la fonction de convolution ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_image = tf.reshape(X, [-1, 96, 128, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(X_image, w_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, w_conv2) + b_conv2)\n",
    "h_pool1 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# uncomment if we want to normalize the convolution\n",
    "\n",
    "# h_norm1 = tf.nn.local_response_normalization(h_pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a dropout\n",
    "h_pool1_drop = tf.nn.dropout(h_pool1, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_fc1 = weight_variable([48 * 64 * 64, 128])\n",
    "b_fc1 = bias_variable([128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous redimensionnons afin d'applatir totalement notre image et faire une couche fully connected (lié à la dimension des poids ci-dessus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_pool1_drop_flat = tf.reshape(h_pool1_drop, [-1, 48*64*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool1_drop_flat, w_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A new dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_fc2 = weight_variable([128, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Il faut maintenant définir le loss, la technique de descente de gradient, l'accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous chargeons alors les données d'apprentissage plus tôt enregistrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shelve\n",
    "\n",
    "d = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/train')\n",
    "X_train = d['X_train']\n",
    "y_train = d['y_train']\n",
    "d.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoding pour les sorties et normalisation des pixel pour les X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train, dtype=np.uint8)\n",
    "y_train = oneHotEncoding(y_train, 10)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une base de train et de test (validation) pour controler la descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_test, y_test = split_validation_set(X_train, y_train, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancement de l'apprentissage (on constate que la descente de gradient ne marche pas très bien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0  training accuracy  0.16\n",
      "step  10  training accuracy  0.2\n",
      "step  20  training accuracy  0.1\n",
      "step  30  training accuracy  0.1\n",
      "step  40  training accuracy  0.12\n",
      "step  50  training accuracy  0.16\n",
      "step  60  training accuracy  0.08\n",
      "step  70  training accuracy  0.12\n",
      "step  80  training accuracy  0.18\n",
      "step  90  training accuracy  0.24\n",
      "step  100  training accuracy  0.14\n",
      "step  110  training accuracy  0.12\n",
      "step  120  training accuracy  0.16\n",
      "step  130  training accuracy  0.1\n",
      "step  140  training accuracy  0.06\n",
      "step  150  training accuracy  0.12\n",
      "step  160  training accuracy  0.16\n",
      "step  170  training accuracy  0.12\n",
      "step  180  training accuracy  0.14\n",
      "step  190  training accuracy  0.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "for i in range(200):\n",
    "    batch = next_batch(X_train, y_train, batch_size)\n",
    "    if i%10 == 0:\n",
    "        train_accuracy = accuracy.eval(\n",
    "            feed_dict={X:batch[0], y_:batch[1]})\n",
    "        print \"step \",str(i),\" training accuracy \", str(train_accuracy)\n",
    "    train_step.run(feed_dict={X:batch[0], y_:batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can delete X_train and y_train now\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "########Chargement des données de test#######\n",
    "#############################################\n",
    "import shelve\n",
    "\n",
    "d = shelve.open('/home/maxence/Documents/kaggle/State_Farm_Distracted_Driver_Detection/imgs/test')\n",
    "X_test = d['X_test']\n",
    "X_test_id = d['X_test_id']\n",
    "d.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je boucle en sélectionneant 100 par 100 les données de test sinon j'ai une erreur de out of memory vu la taille du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in range(100,len(X_test), 100):\n",
    "    try:\n",
    "        pred = np.concatenate((pred, y_conv.eval(feed_dict={X:X_test[j:i]})))\n",
    "    except:\n",
    "        pred = y_conv.eval(feed_dict={X:X_test[j:i]})\n",
    "    j=i\n",
    "\n",
    "if(len(pred)!=len(X_test)):\n",
    "    pred = np.concatenate((pred, y_conv.eval(feed_dict={X:X_test[i:]})))\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du fichier de soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(pred, X_test_id, 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test, X_test_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
